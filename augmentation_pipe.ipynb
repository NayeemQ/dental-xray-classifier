{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"165L2xQ87SxIiH3kbP8a7dNRbDSG743yA","authorship_tag":"ABX9TyO2Hx+2KxnKZ7gUoZMLAWaS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o3HvwzAoM4d1","executionInfo":{"status":"ok","timestamp":1746366366484,"user_tz":-330,"elapsed":43037,"user":{"displayName":"Nayeem Quraishi","userId":"04116393662634512757"}},"outputId":"0d061f88-6783-4396-e043-bbac646cc6db"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow\n","  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Collecting astunparse>=1.6.0 (from tensorflow)\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n","Collecting flatbuffers>=24.3.25 (from tensorflow)\n","  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Collecting google-pasta>=0.1.1 (from tensorflow)\n","  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n","Collecting libclang>=13.0.0 (from tensorflow)\n","  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n","Collecting tensorboard~=2.19.0 (from tensorflow)\n","  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n","  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n","  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n","Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n","  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m588.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n","Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n","Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm.auto import tqdm\n","\n","# Set random seeds for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# Load the data\n","def load_data(data_dir='/content/drive/MyDrive/Colab Notebooks/processed_data'):\n","    X_train = np.load(os.path.join(data_dir, 'X_train.npy'))\n","    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n","    X_valid = np.load(os.path.join(data_dir, 'X_valid.npy'))\n","    y_valid = np.load(os.path.join(data_dir, 'y_valid.npy'))\n","    X_test = np.load(os.path.join(data_dir, 'X_test.npy'))\n","    y_test = np.load(os.path.join(data_dir, 'y_test.npy'))\n","\n","    with open(os.path.join(data_dir, 'class_names.pkl'), 'rb') as f:\n","        class_names = pickle.load(f)\n","\n","    return X_train, y_train, X_valid, y_valid, X_test, y_test, class_names\n","\n","# Analyze class distribution\n","def analyze_class_distribution(y_train, class_names):\n","    unique_classes, counts = np.unique(y_train, return_counts=True)\n","    class_distribution = dict(zip([class_names[i] for i in unique_classes], counts))\n","\n","    print(\"Class distribution in training data:\")\n","    for class_name, count in class_distribution.items():\n","        print(f\"{class_name}: {count}\")\n","\n","    return class_distribution\n","\n","# Visualize sample images\n","def visualize_samples(X_train, y_train, class_names, num_samples=3):\n","    fig, axes = plt.subplots(len(class_names), num_samples, figsize=(10, 12))\n","\n","    for class_idx, class_name in enumerate(class_names):\n","        # Get indices of samples belonging to this class\n","        indices = np.where(y_train == class_idx)[0]\n","\n","        # If there are not enough samples, use what's available\n","        samples_to_show = min(num_samples, len(indices))\n","\n","        for i in range(samples_to_show):\n","            sample_idx = indices[i]\n","            img = X_train[sample_idx].reshape(64, 64)\n","            axes[class_idx, i].imshow(img, cmap='gray')\n","            axes[class_idx, i].set_title(f\"{class_name}\")\n","            axes[class_idx, i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig('sample_images.png')\n","    plt.close()\n","\n","def safe_dental_contrast(image):\n","    \"\"\"Numerically stable contrast adjustment for dental X-rays\"\"\"\n","    # Convert to float32\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","\n","    # CLAHE-like contrast with safe ranges\n","    image = tf.image.adjust_contrast(\n","        image,\n","        contrast_factor=tf.clip_by_value(\n","            tf.random.uniform([], 0.85, 1.15),  # More conservative range\n","            0.8, 1.2  # Absolute safety bounds\n","        )\n","    )\n","\n","    # Safe brightness adjustment\n","    image = tf.image.adjust_brightness(\n","        image,\n","        delta=tf.clip_by_value(\n","            tf.random.normal([], mean=0.0, stddev=0.05),\n","            -0.1, 0.1\n","        )\n","    )\n","\n","    # Ensure valid pixel range\n","    return tf.clip_by_value(image, 0.0, 1.0)\n","\n","\n","# Create augmentation generator\n","def create_augmentation_generator():\n","    return ImageDataGenerator(\n","        rotation_range=15,\n","        width_shift_range=0.05,\n","        height_shift_range=0.05,\n","        shear_range=0.05,\n","        zoom_range=[0.9, 1.1],\n","        horizontal_flip=True,\n","        vertical_flip=False,\n","        fill_mode='constant',\n","        cval=0.0,\n","        preprocessing_function=safe_dental_contrast,\n","        brightness_range=None\n","    )\n","\n","# Generate augmented samples for minority classes\n","def generate_augmented_data(X_train, y_train, class_distribution, class_names, target_samples=10000):\n","    augmented_X = []\n","    augmented_y = []\n","\n","    datagen = create_augmentation_generator()\n","\n","    # Add all original samples to the augmented dataset\n","    augmented_X.extend(X_train)\n","    augmented_y.extend(y_train)\n","\n","    # For each class that needs augmentation\n","    for class_idx, class_name in enumerate(class_names):\n","        # Get count of this class\n","        if class_name in class_distribution:\n","            class_count = class_distribution[class_name]\n","        else:\n","            continue\n","\n","        # Skip majority class or classes with sufficient samples\n","        if class_count >= target_samples:\n","            print(f\"Skipping {class_name} (already has {class_count} samples)\")\n","            continue\n","\n","        # Calculate how many augmented samples we need\n","        num_to_generate = target_samples - class_count\n","        print(f\"Generating {num_to_generate} augmented samples for {class_name}\")\n","\n","        # Get indices of samples belonging to this class\n","        class_indices = np.where(y_train == class_idx)[0]\n","\n","        # Generate augmented samples\n","        samples_generated = 0\n","        while samples_generated < num_to_generate:\n","            # Randomly select a sample from this class\n","            sample_idx = np.random.choice(class_indices)\n","            sample = X_train[sample_idx].reshape(1, 64, 64, 1)\n","\n","            # Generate an augmented sample\n","            for x_batch in datagen.flow(sample, batch_size=1):\n","                augmented_X.append(x_batch[0])\n","                augmented_y.append(class_idx)\n","                samples_generated += 1\n","\n","                if samples_generated >= num_to_generate:\n","                    break\n","\n","    # Convert lists to numpy arrays\n","    augmented_X = np.array(augmented_X)\n","    augmented_y = np.array(augmented_y)\n","\n","    return augmented_X, augmented_y\n","\n","# Visualize augmented samples\n","def visualize_augmented_samples(original_sample, class_name):\n","    datagen = create_augmentation_generator()\n","\n","    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n","    axes[0, 0].imshow(original_sample.reshape(64, 64), cmap='gray')\n","    axes[0, 0].set_title(\"Original\")\n","    axes[0, 0].axis('off')\n","\n","    sample = original_sample.reshape((1,) + original_sample.shape)\n","\n","    i = 0\n","    for batch in datagen.flow(sample, batch_size=1):\n","        i += 1\n","        row, col = divmod(i, 3)\n","        if row == 0 and col == 0:\n","            continue  # Skip the first position as it's the original\n","\n","        axes[row, col].imshow(batch[0].reshape(64, 64), cmap='gray')\n","        axes[row, col].set_title(f\"Aug {i}\")\n","        axes[row, col].axis('off')\n","\n","        if i >= 8:  # Show 8 augmented samples + original\n","            break\n","\n","    plt.suptitle(f\"Augmentation examples for class: {class_name}\")\n","    plt.tight_layout()\n","    plt.savefig(f'augmentation_example_{class_name}.png')\n","    plt.close()\n","\n","# Save augmented dataset\n","def save_augmented_dataset(X_train_aug, y_train_aug, X_valid, y_valid, X_test, y_test, output_dir='/content/drive/MyDrive/Colab Notebooks/augmented_data'):\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    np.save(os.path.join(output_dir, 'X_train_augmented.npy'), X_train_aug)\n","    np.save(os.path.join(output_dir, 'y_train_augmented.npy'), y_train_aug)\n","    np.save(os.path.join(output_dir, 'X_valid.npy'), X_valid)\n","    np.save(os.path.join(output_dir, 'y_valid.npy'), y_valid)\n","    np.save(os.path.join(output_dir, 'X_test.npy'), X_test)\n","    np.save(os.path.join(output_dir, 'y_test.npy'), y_test)\n","\n","    print(f\"Augmented dataset saved to {output_dir}\")\n","\n","def main():\n","    # Load data\n","    print(\"Loading data...\")\n","    X_train, y_train, X_valid, y_valid, X_test, y_test, class_names = load_data()\n","\n","    # Analyze original class distribution\n","    print(\"\\nAnalyzing class distribution...\")\n","    class_distribution = analyze_class_distribution(y_train, class_names)\n","\n","    # Visualize samples from each class\n","    print(\"\\nVisualizing sample images...\")\n","    visualize_samples(X_train, y_train, class_names)\n","\n","    # Show augmentation examples for each minority class\n","    print(\"\\nVisualizing augmentation examples...\")\n","    minority_classes = [\"Cavity\", \"Impacted Tooth\", \"Implant\"]\n","    for class_name in minority_classes:\n","        class_idx = class_names.index(class_name)\n","        sample_idx = np.where(y_train == class_idx)[0][0]\n","        visualize_augmented_samples(X_train[sample_idx], class_name)\n","\n","    # Set target samples per class for balancing\n","    # Aim for a more balanced distribution without making dataset too large\n","    target_samples = 10000  # Adjust based on your memory constraints\n","\n","    # Generate augmented data\n","    print(f\"\\nGenerating augmented data (target: {target_samples} samples per class)...\")\n","    X_train_aug, y_train_aug = generate_augmented_data(\n","        X_train, y_train, class_distribution, class_names, target_samples\n","    )\n","\n","    # Analyze augmented class distribution\n","    print(\"\\nClass distribution after augmentation:\")\n","    unique_classes, counts = np.unique(y_train_aug, return_counts=True)\n","    for i, count in zip(unique_classes, counts):\n","        print(f\"{class_names[i]}: {count}\")\n","\n","    # Save augmented dataset\n","    print(\"\\nSaving augmented dataset...\")\n","    save_augmented_dataset(X_train_aug, y_train_aug, X_valid, y_valid, X_test, y_test)\n","\n","    print(\"\\nData augmentation completed successfully!\")\n","    print(f\"Original training set: {X_train.shape[0]} samples\")\n","    print(f\"Augmented training set: {X_train_aug.shape[0]} samples\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mp-8KxFbs7Yd","executionInfo":{"status":"ok","timestamp":1746366616556,"user_tz":-330,"elapsed":130553,"user":{"displayName":"Nayeem Quraishi","userId":"04116393662634512757"}},"outputId":"14cd5512-abbc-4c37-f571-bc4914059b1e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","\n","Analyzing class distribution...\n","Class distribution in training data:\n","Cavity: 3343\n","Fillings: 5262\n","Impacted Tooth: 1032\n","Implant: 1784\n","Normal: 17116\n","\n","Visualizing sample images...\n","\n","Visualizing augmentation examples...\n","\n","Generating augmented data (target: 10000 samples per class)...\n","Generating 6657 augmented samples for Cavity\n","Generating 4738 augmented samples for Fillings\n","Generating 8968 augmented samples for Impacted Tooth\n","Generating 8216 augmented samples for Implant\n","Skipping Normal (already has 17116 samples)\n","\n","Class distribution after augmentation:\n","Cavity: 10000\n","Fillings: 10000\n","Impacted Tooth: 10000\n","Implant: 10000\n","Normal: 17116\n","\n","Saving augmented dataset...\n","Augmented dataset saved to /content/drive/MyDrive/Colab Notebooks/augmented_data\n","\n","Data augmentation completed successfully!\n","Original training set: 28537 samples\n","Augmented training set: 57116 samples\n"]}]}]}